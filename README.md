A Smartphone-Based Mobility Assistant Using Depth Imaging for Visually Impaired and Blind
This Final Year Project aims to empower visually impaired individuals by turning a smartphone into an intelligent mobility assistant. By combining object detection, depth estimation, and real-time auditory feedback, the system helps users navigate their surroundings more safely and independently.

Key Features
Real-time Object Detection: Identifies common obstacles and objects using a trained machine learning model.

Depth Estimation: Calculates the distance of objects from the user to provide spatial awareness.

Voice Feedback: Offers instant audio guidance to alert users about nearby obstacles.

Mobile-Friendly: Designed to run on smartphones for convenient, everyday use.

Accessibility First: Built with a focus on usability and safety for the visually impaired.

Tech Stack
Python, OpenCV, TensorFlow / PyTorch

Mobile integration (e.g., Android with Java/Kotlin or Flutter)

Text-to-Speech APIs for auditory feedback

Depth imaging via stereo camera or smartphone depth sensors

Goals
Improve independent mobility for the blind and visually impaired

Deliver accurate, real-time detection and navigation assistance

Leverage affordable smartphone hardware for wide accessibility
